---
title: "Lab 07: What makes a song more positive?"
author: "The A Team: Naomi Rubin, Jwalin Patel, Annie Sawers, Alex Williams, JM Stroh"
date: "3-23-21"
output: 
  pdf_document: 
    fig_height: 4
    fig_width: 6
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo  =  TRUE,
                      warning = FALSE, 
                      message = FALSE)
```

## Load packages & data

```{r load-packages, message = FALSE}
library(tidyverse)
library(broom)
library(knitr)
library(rms)
```

```{r load-data, message = FALSE}
spotify <- read_csv("data/spotify-popular.csv") %>%
  mutate(key = factor(key), 
         mode = factor(mode))
```

## Exercise 1

```{r}
full_model <- lm(valence ~ danceability + energy + key + loudness +
                   mode + speechiness + acousticness + instrumentalness +
                   liveness + tempo + duration_ms + playlist_genre,
                 data=spotify)

tidy(full_model)%>%
  kable(digits = 3)
```

```{r}
int_only_model <- lm(valence ~ 1, data = spotify)
tidy(int_only_model)%>%
  kable(digits=3)
```


## Exercise 2

```{r results = "hide"}
backward_aic <- step(full_model, direction="backward", results = "hide")
```

```{r}
tidy(backward_aic)%>%
  kable(digits=3)
```

## Exercise 3

```{r}
## number of observations
n <- nrow(spotify)
```


```{r results = "hide"}
backward_bic <- step(full_model, direction="backward", k=log(n), results = "hide")
```

```{r}
tidy(backward_bic)%>%
  kable(digits=3)
```

## Exercise 4

The models do not have the same number of predictors. The model using AIC has 
all of the same predictors as the model using BIC plus an additional five 
predictor variables.

This is the model we would expect to have more predictors because for data with 
more than eight observations, like the spotify data which has 508 observations,
the penalty for BIC is larger than that of AIC. This means that BIC tends to 
favor more parsimonious models (i.e. models with fewer terms). Therefore, we 
would expect the model using BIC to have fewer predictors, and this is in fact 
the case.

## Exercise 5

```{r}
forward_aic <- step(int_only_model,formula(full_model),direction = "forward",results = "hide")
```

```{r}
tidy(forward_aic)%>%
  kable(digits=3)
```

## Exercise 6

```{r}
forward_bic <- step(int_only_model,formula(full_model),direction="forward",k=log(n),results="hide")
```

```{r}
tidy(forward_bic)%>%
  kable(digits=3)
```

## Exercise 7

```{r}
glance(forward_aic) %>% select(r.squared, adj.r.squared)
```

```{r}
glance(forward_bic) %>% select(r.squared, adj.r.squared)
```
In general, we choose the model with the highest adjusted $R^2$ value, i.e. we choose the regression model that explains the most amount of variation in the response, given the correction adjusted $R^2$ performs for the number of predictor variables. Thus, we would choose the forward_aic model.  

## Exercise 8 
```{r}
selected_aug<- augment(forward_aic)
selected_aug <- selected_aug %>%
  mutate(obs_num = 1:nrow(selected_aug))

ggplot(data = selected_aug, aes(x = .fitted, y = .std.resid)) + geom_point() +
  labs(x = "Predicted Value", y = "Standardized Residuals", 
       title = "Standardized Residuals vs Predicted") + 
  geom_hline(yintercept = -3,color = "red",linetype = "dotted") +
  geom_hline(yintercept = 3,color = "red",linetype = "dotted") 
```
Based on the above plot of standardized plot vs residuals, I am not sure that 
the linearity assumption is met. residuals appear to be centered and most varied
at about predicted value .5, and then get fewer and smaller as you move away from
that value in either direction. 